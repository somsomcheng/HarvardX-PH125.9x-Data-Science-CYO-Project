---
title: 'HarvardX PH125.9x Data Science CYO Project'
author: "Cheng Ming"
date: "5/17/2020"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# 1. Overview of the project
  This report is the second Capstone Project of "HarvardX PH125.9x Data Science: Capstones". In this project, I'll use several advanced regression techniques to predict house prices in NYC, using the datasets from Kaggle. The datasets can be downloaded from:  https://www.kaggle.com/c/house-prices-advanced-regression-techniques. Two datasets are provided on the website: train set and test set. Since the test does not contrain SalePrice variable, we'll only use the train set so that we can calculate RMSLE. This "train set" will be separated into training set and testing set for this project. I'll use the training set to train the machine learning algorithm and use the testing set to evaluate our models.
  As the scale of the house prices is too large, we'll use Root Mean Squared Logarithmic Error (RMSLE) to evaluate the accuracy of the predicted results, instead of using RMSE. In a nutshell, the objective of this project is to predict the value of SalePrice using other variables in the dataset while minimize the RMSLE. 

# 2. Steps of the project
  The following steps will be performed: 
  (1) Data Cleaning: Load and mutate the dataset
  (2) Data EXploration and Visualization: Understand the dataset structure with an exploratory data analysis
  (3) Machine Learning Algorithms Evaluation: Train a machine learning algorithm to predict SalePrice
  (4) Results Analysis: Compare RMSLE derived from different models

## 2.1 Data Cleaning: Load and mutate the dataset
  The first step is to load relevant packages needed. To make this report more succinct, I've hiden this r chunk.
```{r include=FALSE}
if(!require(bsts)) install.packages("bsts")
if(!require(e1071)) install.packages("e1071")
if(!require(knitr)) install.packages("knitr")
if(!require(glmnet)) install.packages("glmnet")
if(!require(caTools)) install.packages("caTools")
if(!require(corrplot)) install.packages("corrplot")
if(!require(lubridate)) install.packages("lubridate")
if(!require(data.table)) install.packages("data.table")
if(!require(kableExtra)) install.packages("kableExtra")
if(!require(randomForest)) install.packages("randomForest")
if(!require(RColorBrewer)) install.packages("RColorBrewer")
library(bsts)
library(dplyr)
library(caret)
library(e1071) 
library(knitr)
library(scales)
library(glmnet) 
library(caTools) 
library(ggplot2)
library(ggthemes)
library(corrplot)
library(lubridate) 
library(tidyverse)
library(data.table)
library(kableExtra)
library(randomForest)
library(RColorBrewer)
 
# Load the dataset
train<-read.csv("train.csv")
set.seed(123, sample.kind="Rounding")

# Convert NA to numeric 0 for modeling purposes
train_set<-train%>% mutate_all(~replace(., is.na(.), 0))

#Converting character variables to numeric
train$regshape[train$LotShape == "Reg"] <- 1
train$regshape[train$LotShape != "Reg"] <- 0
train$paved[train$Street == "Pave"] <- 1 
train$paved[train$Street != "Pave"] <- 0

# Separate the data set into two sets: training_set and testing_set
test_index<-createDataPartition(y = train_set$SalePrice, times = 1, p = 0.1, list = FALSE)
training_set<-train_set[-test_index,]
testing_set<-train_set[test_index,]



# Take a glance at the training_set
summary(training_set)

```

## 2.2 Data Exploration and Visualization: Understand the dataset structure with an exploratory data analysis
  Let's take a look at how is the SalePrice distributed
```{r}
ggplot(data=train, aes(x=SalePrice)) +
  geom_histogram(fill="skyblue2", binwidth = 2000) +
  scale_x_continuous(breaks= seq(0, 1000000, by=90000),labels = comma) +
  labs(title="House Sale Prices Distribution")+
  theme_minimal()
```
  
  We can see that SalePrice in the dataset is right skewed. 

  There're 80 more variables in the dataset other than SalePrice, how should we know which variables are useful for predicting SalePrice? A correlation matrix is helpful for solving such query.
```{r}
# Identify index vector of numeric variables
numeric_vars<- which(sapply(training_set, is.numeric)) 

# Identify the column names of numeric variables
numeric_vars_col<-data.table(names(numeric_vars))  
temp<-training_set[,numeric_vars]

# Get the correlations of all numeric variables
corr_all<-cor(temp,use="pairwise.complete.obs")

# Sort the correlations with SalePrice on a descending order
corr_sorted<-as.matrix(sort(corr_all[,'SalePrice'], decreasing = TRUE)) 

# Select high corelations only
high_corr<-names(which(apply(corr_sorted, 1, function(x) abs(x)>0.5))) 
high_corr_col<-data.table(high_corr)
temp2<- corr_all[high_corr, high_corr]

# Plot the correlations
corrplot(temp2, addrect = 2, type = "lower", col = brewer.pal(n = 8, name = "RdBu"))
```

  As shown in the correlation matrix table above, there're 10 numeric variables in the training dataset that have correlations above 0.5. Among these 10 numeric variables, OverallQual has the highest correlation of 0.8. We can see from the box plot below that there's indeed an obvious correlation between OverallQual and SalePrice.
  
```{r}
training_set%>%ggplot(aes(x=factor(OverallQual),y=SalePrice))+
  geom_boxplot(col='coral1')+ 
  labs(x='OverallQual')+ 
  scale_y_continuous(breaks= seq(0, 1000000, by=90000),labels = comma)+ 
  labs(title="Corrrelation between OverallQual and SalePrice")+
  theme_minimal()

```
  And the distribution of SalePrice by OverallQual is shown below:
```{r}
training_set%>%ggplot(aes(x = SalePrice,fill = as.factor(OverallQual)))+
  geom_histogram(position = "stack", binwidth = 10000)+
  ggtitle("Distribution of SalePrice by OverallQual")+
  ylab("Count")+
  xlab("SalePrice")+ 
  scale_x_continuous(breaks= seq(0, 1000000, by=90000),labels = comma)+
  scale_fill_discrete(name="OverallQual")+
  theme(plot.title = element_text(hjust = 0.1), 
        legend.position=c(0.9,0.7),
        legend.background = element_rect(fill="grey90",size=0.5,linetype="solid"))                      
                                                                                
```
  
  
  
  
## 2.3 Machine Learning Algorithms Evaluation: Train a machine learning algorithm to predict SalePrice
#### Model 1: Multiple Linear Regression Model
  First, we'll use the 10 most relavant numeric variables identified previously to run a Multiple Linear Regression model. Using this model, we'll then predict SalePrice in the testing set.
```{r}
# Run the Multiple Linear Regression Model
fit<-lm(SalePrice~OverallQual+GrLivArea+GarageCars+GarageArea+TotalBsmtSF+X1stFlrSF+FullBath+TotRmsAbvGrd+YearBuilt+YearRemodAdd,data=training_set)

summary(fit)
```

```{r}
# Predict SalePrice in testing set using the model
SalePrice_prediction<-predict(fit,testing_set)

# Scale the data
log_SalePrice_Prediction<-log(SalePrice_prediction)
log_SalePrice_Real<-log(testing_set$SalePrice)

#Calculate RMSLE
RMSLE<-RMSE(log_SalePrice_Prediction,log_SalePrice_Real)
RMSLE
```

#### Model 2: Random Forests Model
  Next, we'll use Random Forests Model to predict the SalePrice. Random forests Model is an ensemble learning method that can be used for classification and regression. Random forests solve the overfitting habit of decision trees.
```{r}
rf<-randomForest(SalePrice~OverallQual+GrLivArea+GarageCars+GarageArea+TotalBsmtSF+X1stFlrSF+FullBath+TotRmsAbvGrd+YearBuilt+YearRemodAdd,data=training_set)
SalePrice_prediction2<-predict(rf,testing_set)

log_SalePrice_Prediction2<-log(SalePrice_prediction2)
log_SalePrice_Real<-log(testing_set$SalePrice)
RMSLE2<- RMSE(log_SalePrice_Real,log_SalePrice_Prediction2)
RMSLE2
```
 The graph below plots the relationship between log predicted SalePrice and log actual Saleprice
```{r}
plot(log_SalePrice_Prediction2,log_SalePrice_Real,main = "log Predicted SalePrice VS. log Actual Saleprice")+abline(0,1)
```
 

## 2.4 Results Analysis: Compare RMSLE results derived from different models
```{r}
bind_rows(tibble(Method="Multiple Linear Regression Model",RMSLE=RMSE(log_SalePrice_Prediction,log_SalePrice_Real)),
          tibble(Method="Random Forests Model",RMSLE=RMSE(log_SalePrice_Real,log_SalePrice_Prediction2)))
```

# 3. Conclusions
  Based on the two RMSLE derived above, we know that Random Forests Model is better than Multiple Linear Regression Model at predcting SalePrice, using the top 10 most relevant variables from the dataset. 
  
# 4. Limitations
  In our model, only the top 10 most relevant numeric variables are used to predict SalePrice. However, thereâ€™re many other factors that may affect SalePrice, such as YrSold(Year Sold) and SaleCondition(Condition of sale). More variables can be included in a futher analysis. 
   

  My Harvard_Data_Science_House Prices Github repository can be found at https://github.com/somsomcheng/HarvardX-PH125.9x-Data-Science-CYO-Project
  
  
  ## Thank you so much for spending time reading my project! Really appreciate it :)
